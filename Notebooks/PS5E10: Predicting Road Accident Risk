{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91721,"databundleVersionId":13760552,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/drmclandgraf/ps5e10-predicting-road-accident-risk?scriptVersionId=271254824\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<h1>Predicting Road Accident Risk</h1> \n\nWelcome to my notebook for this playground challenge. I will try to... \n- Make some initial data exploration \n- Exploring datatypes and distribution \n- Exploring connections between the variables\n- Train a suitable model\n- Explore, which variables make the most impact\n\n<h3>Initial Setup</h3> ","metadata":{}},{"cell_type":"code","source":"# This is the standard code, when creating a new notebook on Kaggle with some extra libraries loaded\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import mean_squared_error\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:44:39.628746Z","iopub.execute_input":"2025-10-27T10:44:39.629159Z","iopub.status.idle":"2025-10-27T10:44:42.818372Z","shell.execute_reply.started":"2025-10-27T10:44:39.629128Z","shell.execute_reply":"2025-10-27T10:44:42.817231Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"At first, we load the train and test data.\nTo get a first look at the data, we print the first few rows.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/playground-series-s5e10/train.csv\")\ntest_submission = pd.read_csv(\"/kaggle/input/playground-series-s5e10/test.csv\")\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:44:42.819962Z","iopub.execute_input":"2025-10-27T10:44:42.820574Z","iopub.status.idle":"2025-10-27T10:44:44.525985Z","shell.execute_reply.started":"2025-10-27T10:44:42.820538Z","shell.execute_reply":"2025-10-27T10:44:44.525025Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h3>First look at the data</h3> \nWe can already see, that we have different types of data. \"road_type\" is categorical, \"num_lanes\" is numerical, and \"road_signs_present\" is boolean.\nWe further can already guess, that our numerical data is on different scales.\n\"num_lanes\" describing the number of lanes can only have discreet values, starting at 1. It will also probably have a rather small upper limit. The speed limit also only has certain predefined values, while curvature only can be a number between 0 and 1. <br>\nWe can also look print the different types of data we have in our dataset.","metadata":{}},{"cell_type":"code","source":"train.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:44:44.526992Z","iopub.execute_input":"2025-10-27T10:44:44.527534Z","iopub.status.idle":"2025-10-27T10:44:44.535492Z","shell.execute_reply.started":"2025-10-27T10:44:44.527499Z","shell.execute_reply":"2025-10-27T10:44:44.534459Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Again, we can see our different datatypes, we have numerical (int64 and float64; float64 can have decimals), categorical (object) and boolian (bool; meaning True or False are possible values). \n<br>\nLets take a look at how the data is distributed, at first the numerical values.","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:44:44.538388Z","iopub.execute_input":"2025-10-27T10:44:44.53875Z","iopub.status.idle":"2025-10-27T10:44:44.686982Z","shell.execute_reply.started":"2025-10-27T10:44:44.538716Z","shell.execute_reply":"2025-10-27T10:44:44.685822Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Again, we see curvature and accident_risk are between 0 and 1. There are up to 4 lanes, and the speed limit can be between 25 and 70mph. <br>\nThe number of reported accidents range from 0 to 7. <br>\nNow lets look at our categorical data. As \"num_lanes\", \"speed_limit\", \"num_reported_accidents\" only can take on discreet values, we again look at how they are distributed.","metadata":{}},{"cell_type":"code","source":"for column in train.select_dtypes(include=['object', \"boolean\"]).columns:\n    display(train[column].value_counts())\n\ndisplay(train[\"num_lanes\"].value_counts())\ndisplay(train[\"speed_limit\"].value_counts())\ndisplay(train[\"num_reported_accidents\"].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:44:44.688007Z","iopub.execute_input":"2025-10-27T10:44:44.688354Z","iopub.status.idle":"2025-10-27T10:44:44.930951Z","shell.execute_reply.started":"2025-10-27T10:44:44.688322Z","shell.execute_reply":"2025-10-27T10:44:44.92968Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Most the categories are of roughly equal size. That's good! If some categories only had a few entries, most models would have a hard time learning this category and we might have to resort to stratified sampling or weighing the data. <br>\nSpeed_limit has a few less entries at speed limit of 70, but thats still plenty. <br>\nFor the number of reported accidents, there are only few places with 4 or more accidents. This might become a problem, depending on how we treat the variable. If we decide to treat it as categorical, we have only 2 examples for the last category. We could collapse some categories, for example all with 3 or more reported accidents. But we can also just treat this variable as if it would be numerical. <br>\nNow lets take another look at the distribution of our two final numerical variables, curvature and accident risk, by plotting them.","metadata":{}},{"cell_type":"code","source":"sns.kdeplot(x = train[\"curvature\"], fill = True)\nplt.title(\"curvature\")\nplt.show()\nsns.kdeplot(x = train[\"accident_risk\"], fill = True) \nplt.title(\"accident_risk\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:44:44.932012Z","iopub.execute_input":"2025-10-27T10:44:44.932292Z","iopub.status.idle":"2025-10-27T10:44:49.473266Z","shell.execute_reply.started":"2025-10-27T10:44:44.932254Z","shell.execute_reply":"2025-10-27T10:44:49.472225Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can see that both are not really normally distributed.\nWhile this in itself is not a problem it could become one depending on how we choose to model the risk and how we include the curvature. <br>\n\n<h3>Exploring Associations between Variables</h3> \n\nIn the next step, we will look at how different variables are distributed in connection to each other. <br>\nFor this, we at first split the data into different sets, a training, a validation, and a test set (A different then the one we use for our submissions). I use the training set to train a model and tune hyperparameters, the validation set to explore the variables and compare different models and strategies, and the test set to estimate how well the final model performs. <br>\nWe split up the data before we look at how our dependent variable (accident risk) is connected to other variables to avoid data leakage. <br>","metadata":{}},{"cell_type":"code","source":"train = train.drop(\"id\", axis = 1)\ntrain, test = train_test_split(train, test_size = 0.2, random_state = 13)\ntrain, val = train_test_split(train, test_size = 0.25, random_state = 13)\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:44:49.474382Z","iopub.execute_input":"2025-10-27T10:44:49.474769Z","iopub.status.idle":"2025-10-27T10:44:49.730136Z","shell.execute_reply.started":"2025-10-27T10:44:49.474744Z","shell.execute_reply":"2025-10-27T10:44:49.729085Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now it is time to explore associations between the different variables. At first we might want to look at correlations.","metadata":{}},{"cell_type":"code","source":"s = ((train.dtypes == \"object\") | (train.dtypes ==  \"bool\"))\nobject_cols = list(s[s].index)\nsns.heatmap(train.drop(object_cols, axis =1).corr().round(2), annot = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:44:49.731076Z","iopub.execute_input":"2025-10-27T10:44:49.731539Z","iopub.status.idle":"2025-10-27T10:44:50.080468Z","shell.execute_reply.started":"2025-10-27T10:44:49.731502Z","shell.execute_reply":"2025-10-27T10:44:50.079419Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see a rather high correlation between curvature and accident_risk as well as speed_limit and accident risk, these seem like good predictors. <b>\nWe do, however, have quite a few categorical variables in our dataset, so we should examine these instead. For them, the correlation-coefficient does not make a lot of sense to use, but we can use different effect sizes, like the correlation ratio and Cramers V. <br>\nCorrelation ratios compares the sum of squares between groups and within groups, larger numbers meaning there is more variation between groups than within them. <br>\nCramers V shows the association between two nominal variables, with larger numbers meaning a higher association. <br>\nWe will calculate all these to get an estimation for where there are possible associations between variables, and between the predictorvariables and the outcome (accident risk).","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom scipy import stats\nimport itertools\n# from https://towardsdatascience.com/a-definitive-guide-to-effect-size-9bc93f00db86/ by Eryk Lewinson\ndef get_cramer_v(x, y):\n    n = len(x)\n    cont_table = pd.crosstab(x, y)\n    chi_2 = stats.chi2_contingency(cont_table, correction=False)[0]\n    v = np.sqrt(chi_2 / (n * (np.min(cont_table.shape) - 1)))\n    return v\n\n#code from https://stackoverflow.com/questions/52083501/how-to-compute-correlation-ratio-or-eta-in-python by Kiryl\ndef correlation_ratio(categories, values):\n    categories = np.array(categories)\n    values = np.array(values)\n    \n    ssw = 0\n    ssb = 0\n    for category in set(categories):\n        subgroup = values[np.where(categories == category)[0]]\n        ssw += sum((subgroup-np.mean(subgroup))**2)\n        ssb += len(subgroup)*(np.mean(subgroup)-np.mean(values))**2\n\n    return (ssb / (ssb + ssw))**.5\n\ndef score_maker(data):\n    cols_save = data.columns\n    types = data.dtypes\n    k = len(cols_save)\n    res_df = pd.DataFrame([[None]*k for i in range(k)],cols_save, cols_save)\n    for feature1 in cols_save:\n        for feature2 in cols_save:\n            if feature1 == feature2:\n                res_df.loc[feature1, feature2] = 1\n            elif types.loc[feature1] in [\"int64\", \"float64\"]:\n                if types.loc[feature2] in [\"int64\", \"float64\"]:\n                    res_df.loc[feature1, feature2] = train[[feature1, feature2]].corr().iloc[1,0]\n                if types.loc[feature2] in [\"bool\", \"object\"]:\n                    res_df.loc[feature1, feature2] = correlation_ratio(train[feature2], train[feature1])\n            elif types.loc[feature1] in [\"bool\", \"object\"]:\n                if types.loc[feature2] in [\"int64\", \"float64\"]:\n                    res_df.loc[feature1, feature2] = correlation_ratio(train[feature1], train[feature2])\n                if types.loc[feature2] in [\"bool\", \"object\"]:\n                    res_df.loc[feature1, feature2] = get_cramer_v(train[feature2], train[feature1])\n    return(res_df)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:44:50.081663Z","iopub.execute_input":"2025-10-27T10:44:50.082174Z","iopub.status.idle":"2025-10-27T10:44:50.095283Z","shell.execute_reply.started":"2025-10-27T10:44:50.082141Z","shell.execute_reply":"2025-10-27T10:44:50.093862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores = score_maker(train)\nplt.figure(figsize=(13, 13))\nsns.heatmap(scores[scores.columns].astype(float).round(2), annot = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:44:50.098163Z","iopub.execute_input":"2025-10-27T10:44:50.098514Z","iopub.status.idle":"2025-10-27T10:44:58.615408Z","shell.execute_reply.started":"2025-10-27T10:44:50.098459Z","shell.execute_reply":"2025-10-27T10:44:58.614338Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see that there is not a lot of connection between the predictor variables, but we see a few variables which show a decent association with accident risk. <br>\nWe have to mind however, that the other effect sizes we used are not identical to the correlation coefficient, nor to each other, and mean mostly different things. The scales of them are also not perfectly identical so such a comparison as seen above is quite rough and has some shortcomings. <br>\nHowever, the table is an easy overfew of where possible associations could be. <br>\nWe can further explore a few examples. Lets look at road type and the time of day.","metadata":{}},{"cell_type":"code","source":"pd.crosstab(train[\"road_type\"], train[\"time_of_day\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:44:58.616438Z","iopub.execute_input":"2025-10-27T10:44:58.61677Z","iopub.status.idle":"2025-10-27T10:44:58.687107Z","shell.execute_reply.started":"2025-10-27T10:44:58.616745Z","shell.execute_reply":"2025-10-27T10:44:58.686207Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see again there is no connection between the two variables, which is probably to be expected. <br>\nWe can also look at more meaningful connections, such as accident risk and speed limit.","metadata":{}},{"cell_type":"code","source":"sns.violinplot(x = train[\"speed_limit\"],y =train[\"accident_risk\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:44:58.688141Z","iopub.execute_input":"2025-10-27T10:44:58.688921Z","iopub.status.idle":"2025-10-27T10:44:59.843528Z","shell.execute_reply.started":"2025-10-27T10:44:58.688884Z","shell.execute_reply":"2025-10-27T10:44:59.842576Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see a lower accident risk at speed limits of 25, 35, and 45 mph, but then a jump and higher accident risk at 60 and 70 mph. <br>\nNow it is finally time to make some models and predictions. <br>\nAt first some thoughts about the task: <br>\nThe interesting thing about this task is that we want to predict the risk for an accident, which might suggest a classification task, however we have the probabilities (risk) given, instead of simply 0 and 1. The model should therefore reflect this. A linear regression is not useful, as it could predict values higher than 1 or lower than 0, and a logistic regression would not make use of all the information we have, as we would have to change the outcome to binary values. <br>\nTree-based models, such as a Random Forest could work, even in their regression form, as their predictions are mostly averages of learned values, meaning they should not exceed 0 at the lower end or 1 at the higher end. <br>\n\n### Predicting Accident Risk\n\nGiven that we have many categorical data, CatBoost is an obvious model-choice, as this is well equiped to deal with categorical data. This makes it easy for us, because we don't have to think about encoding. It is also tree-based, so it should stay in the relevant boundaries of the prediction task. However, the model does not \"know\" that it is predicting probabilities, in this case, it only works as a regressor. <br>\nAt first we tune the model to find the best hyperparameters. For this, I used a randomized search, but I commented this cell out, so the notebook runs faster. I have displayed the best hyperparameters below.","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostRegressor\n\n#parameters = {'iterations': [int(x) for x in np.logspace(np.log10(100),np.log10(2000), num = 15)],\n#             'learning_rate': [x for x in np.logspace(np.log10(0.1), np.log10(0.9), num = 10)],\n#             'depth': [4,6,8,10,12],\n#             'l2_leaf_reg': [x for x in np.logspace(np.log10(100),np.log10(2000), num = 15)],\n#             'colsample_bylevel': [x for x in np.logspace(np.log10(0.1), np.log10(0.9), num = 10)]}\n#cat_reg = CatBoostRegressor(loss_function = \"RMSE\", cat_features = object_cols, verbose = 0,\n#                            early_stopping_rounds = 50, random_state = 13)\n#randomized_search_results = cat_reg.randomized_search(param_distributions = parameters, X = train.drop(\"accident_risk\", axis = 1), \n#                                                      y = train[\"accident_risk\"], n_iter = 20, cv = 3)\n\n#randomized_search_results[\"params\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:44:59.844794Z","iopub.execute_input":"2025-10-27T10:44:59.845224Z","iopub.status.idle":"2025-10-27T10:45:00.443674Z","shell.execute_reply.started":"2025-10-27T10:44:59.845194Z","shell.execute_reply":"2025-10-27T10:45:00.441869Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The best parameters were: depth = 6, learning_rate = 0.9, l2_leaf_reg = 686.0826, iterations = 1303. <br>\nNow lets test the model on our validationset.","metadata":{}},{"cell_type":"code","source":"cat_model = CatBoostRegressor(loss_function = \"RMSE\", cat_features = object_cols, verbose = 0,\n                             depth = 6, learning_rate = 0.9, l2_leaf_reg = 686.0826, iterations = 1303,\n                              early_stopping_rounds = 50, random_state = 13)\ncat_model.fit(X = train.drop(\"accident_risk\", axis = 1), y = train[\"accident_risk\"])\nval_predictions = cat_model.predict(val.drop(\"accident_risk\", axis = 1))\nprint(\"The test predictions range from {} to {}.\".format(min(val_predictions), max(val_predictions)))\nrmse = np.sqrt(mean_squared_error(val[\"accident_risk\"], val_predictions))\nprint(\"Estimated RMSE: {}\".format(round(rmse,4)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:45:00.444842Z","iopub.execute_input":"2025-10-27T10:45:00.445362Z","iopub.status.idle":"2025-10-27T10:47:09.324626Z","shell.execute_reply.started":"2025-10-27T10:45:00.445334Z","shell.execute_reply":"2025-10-27T10:47:09.323553Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Our Predictions are in the range from 0 to 1, which is good, and our estimated RMSE also seems good. <br>\n\n### Finding Important Variables\n\nTo make use of our model, we also want to find out which variables are important for the model and (hopefully) are predictive of accident risk ","metadata":{}},{"cell_type":"code","source":"from sklearn.inspection import permutation_importance\nr = permutation_importance(cat_model, val.drop(\"accident_risk\", axis = 1), val[\"accident_risk\"],\n                           n_repeats=30,\n                           random_state=13)\n\nimp = pd.DataFrame({\"Mean\": r[\"importances_mean\"], \"std\": r [\"importances_std\"]}, \n                   index = val.drop(\"accident_risk\", axis = 1).columns)\nimp = imp.sort_values(by = [\"Mean\"], ascending = False)\nplt.figure(figsize=(13,7))\nsns.barplot(y = imp.index,x = imp[\"Mean\"], \n           xerr = imp[\"std\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:47:49.172622Z","iopub.execute_input":"2025-10-27T10:47:49.17295Z","iopub.status.idle":"2025-10-27T10:56:37.157221Z","shell.execute_reply.started":"2025-10-27T10:47:49.172925Z","shell.execute_reply":"2025-10-27T10:56:37.156028Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see that the most important variable for our model and our predictions are the curvature, the speed limit, and lightning. Weather and the number of reported accidents also have some importance, the other variables do not seem important. We can also examine the importance using SHAP values. Let's try it!","metadata":{}},{"cell_type":"code","source":"import shap\nexplainer = shap.Explainer(cat_model, random_state = 13)\nshap_values = explainer(val.drop(\"accident_risk\", axis = 1))\nshap.plots.bar(shap_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:57:01.11947Z","iopub.execute_input":"2025-10-27T10:57:01.120085Z","iopub.status.idle":"2025-10-27T10:57:16.188191Z","shell.execute_reply.started":"2025-10-27T10:57:01.120052Z","shell.execute_reply":"2025-10-27T10:57:16.187165Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The SHAP values are slightly different, but again the most important variables are the same, as well as the unimportant variables are still unimportant. <br>\nIn a real-world example, this insight could be used to make suggestions about how to mitigate risks for accidents. \n\n### Final notes.\n\nFor now, this will be the end of this notebook. There are however many more things which could be done and explored. Further steps could be...\n- Feature Engineering: For example, whether there is a curvature and the speed limit is high might be even more important than the two features seperately, but also bad weather mixed with bad lightning might be especially risky. But we could also try to change our outcome variable, by transforming it.\n- Excluding variables: As seen, some variables seem to be irrelevant for making predictions. We could try to exclude these, which might improve model-performance. (Sidenote: There are also reasons to keep those variables. Tree-based models often have their own method of excluding irrelevant variables when building a tree. But including noise variables (which do not hold any information) can work as a form of regularization and boost model performance)\n- Trying different models: There are many different models we could use to predict the accident risk. We could also try to train a classifier instead of a regression model, which would make more use of the fact, that we predict a probability and do not have a regression model in the strict sense.\n- Making use of the test set: I have split the data into three parts, the first two (train and validation), I use for building a model, examining it, playing around with different variables and models, and choosing a model. The test set is then finally used, to make an estimation on how well the model I have created will work. However, as I have only built one model and did not make any changes to the dataset (except encoding, which CatBoost did on its own), I have no reason to use the testset yet.","metadata":{}},{"cell_type":"markdown","source":"Anyway, thank you for reading my notebook, feel free to leave a comment, any ideas or feedback are highly anticipated.","metadata":{}}]}
